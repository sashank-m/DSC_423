{"metadata":{"kernelspec":{"name":"ir","display_name":"R","language":"R"},"language_info":{"name":"R","codemirror_mode":"r","pygments_lexer":"r","mimetype":"text/x-r-source","file_extension":".r","version":"4.0.5"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":6906058,"sourceType":"datasetVersion","datasetId":3966263}],"dockerImageVersionId":30530,"isInternetEnabled":true,"language":"r","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"data <- read.csv(\"/kaggle/input/online-news-popularity/OnlineNewsPopularity.csv\")","metadata":{"execution":{"iopub.status.busy":"2023-11-21T04:58:28.454399Z","iopub.execute_input":"2023-11-21T04:58:28.457715Z","iopub.status.idle":"2023-11-21T04:58:31.527568Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"library(caret)\nlibrary(MASS)\nlibrary(DAAG)\nlibrary(glmnet)\nlibrary(leaps)\ninstall.packages(\"lmtest\")\nlibrary(lmtest)\nlibrary(dplyr)\nlibrary(glmnet)\nlibrary(stats)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-21T04:58:31.530521Z","iopub.execute_input":"2023-11-21T04:58:31.563858Z","iopub.status.idle":"2023-11-21T04:58:46.947465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"str(data)","metadata":{"execution":{"iopub.status.busy":"2023-11-21T04:58:46.950140Z","iopub.execute_input":"2023-11-21T04:58:46.952111Z","iopub.status.idle":"2023-11-21T04:58:47.022987Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"colnames(data)","metadata":{"execution":{"iopub.status.busy":"2023-11-21T04:58:47.026702Z","iopub.execute_input":"2023-11-21T04:58:47.028710Z","iopub.status.idle":"2023-11-21T04:58:47.050473Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Removing Empty Rows\nhas_zeros <- any(data$n_tokens_content == 0)\nprint(has_zeros)\ndf_no_zeros <- data[data$n_tokens_content != 0, ]\ndata <- df_no_zeros","metadata":{"execution":{"iopub.status.busy":"2023-11-21T04:58:47.053367Z","iopub.execute_input":"2023-11-21T04:58:47.054939Z","iopub.status.idle":"2023-11-21T04:58:47.138246Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Data Cleaning \n\ndata1 <- data\ndata1 <- subset(data1, select = -url)\ndata1 <- subset(data1, select = -timedelta)\ndata1 <- subset(data1, select = -n_non_stop_words)\ndata1 <- subset(data1, select = -LDA_00)\ndata1 <- subset(data1, select = -LDA_02)\ndata1 <- subset(data1, select = -LDA_01)\ndata1 <- subset(data1, select = -LDA_03)\ndata1 <- subset(data1, select = -LDA_04)\ndata1 <- subset(data1, select = -is_weekend)\n\ndata1 <- subset(data1, select = -c(\n    kw_avg_min,\n    kw_avg_max,\n    kw_min_avg, kw_max_avg, kw_avg_avg ,self_reference_avg_sharess,global_sentiment_polarity,global_rate_positive_words,global_rate_negative_words\n))\n\n# Log-transform 'n_non_stop_unique_tokens'\ndata1$n_non_stop_unique_tokens <- log(data1$n_non_stop_unique_tokens + 1)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-21T04:58:47.141424Z","iopub.execute_input":"2023-11-21T04:58:47.143189Z","iopub.status.idle":"2023-11-21T04:58:48.157760Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dim(data1)","metadata":{"execution":{"iopub.status.busy":"2023-11-21T04:58:48.160683Z","iopub.execute_input":"2023-11-21T04:58:48.162150Z","iopub.status.idle":"2023-11-21T04:58:48.178256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating interaction terms based on correlation\ndata1$useful_unique_non_tokens <- data1$n_unique_tokens * data1$n_non_stop_unique_tokens","metadata":{"execution":{"iopub.status.busy":"2023-11-21T04:58:48.181072Z","iopub.execute_input":"2023-11-21T04:58:48.182553Z","iopub.status.idle":"2023-11-21T04:58:48.195866Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#### Assuming your dataframe is named 'data'\ncor_matrix <- cor(data1)\n\n# Set the diagonal to NA to avoid self-correlation\ndiag(cor_matrix) <- NA\n\n# Initialize a dataframe to store the results\nhigh_cor_results <- data.frame()\n\n# Find the indices of the high correlations\nhigh_cor_indices <- which(cor_matrix > 0.7, arr.ind = TRUE)\n\n# Loop through the indices and store the results\nfor (idx in 1:nrow(high_cor_indices)) {\n    row <- high_cor_indices[idx, \"row\"]\n    col <- high_cor_indices[idx, \"col\"]\n    \n    # Avoid duplicates by ensuring row index is less than column index\n    if (row < col) {\n        high_cor_results <- rbind(high_cor_results,\n                                  data.frame(Column1 = colnames(cor_matrix)[row],\n                                             Column2 = colnames(cor_matrix)[col],\n                                             Correlation = cor_matrix[row, col]))\n    }\n}\n\n# Print the high correlation pairs and their correlation values\nprint(high_cor_results)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-21T04:58:48.198846Z","iopub.execute_input":"2023-11-21T04:58:48.200325Z","iopub.status.idle":"2023-11-21T04:58:48.327831Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"    19. kw_min_min:                    Worst keyword (min. shares)\n    20. kw_max_min:                    Worst keyword (max. shares)\n    21. kw_avg_min:                    Worst keyword (avg. shares)\n    22. kw_min_max:                    Best keyword (min. shares)\n    23. kw_max_max:                    Best keyword (max. shares)\n    24. kw_avg_max:                    Best keyword (avg. shares)\n    25. kw_min_avg:                    Avg. keyword (min. shares)\n    26. kw_max_avg:                    Avg. keyword (max. shares)\n    27. kw_avg_avg:                    Avg. keyword (avg. shares)\n    28. self_reference_min_shares:     Min. shares of referenced articles in Mashable\n    29. self_reference_max_shares:     Max. shares of referenced articles in Mashable\n    30. self_reference_avg_sharess:    Avg. shares of referenced articles in Mashable\n    44. global_subjectivity:           Text subjectivity\n    45. global_sentiment_polarity:     Text sentiment polarity\n    46. global_rate_positive_words:    Rate of positive words in the content\n    47. global_rate_negative_words:    Rate of negative words in the content\n       48. rate_positive_words:           Rate of positive words among non-neutral tokens\n    49. rate_negative_words:           Rate of negative words among non-neutral tokens\n    50. avg_positive_polarity:         Avg. polarity of positive words\n    51. min_positive_polarity:         Min. polarity of positive words\n    52. max_positive_polarity:         Max. polarity of positive words\n    53. avg_negative_polarity:         Avg. polarity of negative  words\n    54. min_negative_polarity:         Min. polarity of negative  words\n    55. max_negative_polarity:         Max. polarity of negative  words","metadata":{}},{"cell_type":"code","source":"df_1 <-data1\ndim(df_1)","metadata":{"execution":{"iopub.status.busy":"2023-11-21T04:58:48.330632Z","iopub.execute_input":"2023-11-21T04:58:48.332108Z","iopub.status.idle":"2023-11-21T04:58:48.350876Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"colnames(data1)\ncolnames(df_1)","metadata":{"execution":{"iopub.status.busy":"2023-11-21T04:58:48.354358Z","iopub.execute_input":"2023-11-21T04:58:48.355949Z","iopub.status.idle":"2023-11-21T04:58:48.384276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dim(data)","metadata":{"execution":{"iopub.status.busy":"2023-11-21T04:58:48.388215Z","iopub.execute_input":"2023-11-21T04:58:48.389912Z","iopub.status.idle":"2023-11-21T04:58:48.406433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"simple_model <- lm(shares~., data = df_1)\nsummary(simple_model)","metadata":{"execution":{"iopub.status.busy":"2023-11-21T04:58:48.409132Z","iopub.execute_input":"2023-11-21T04:58:48.410614Z","iopub.status.idle":"2023-11-21T04:58:48.978464Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vif_values <- vif(simple_model)\npredictor_names <- names(simple_model)\nvif_df <- data.frame(Variable = names(vif_values), VIF = format(vif_values, scientific = FALSE))\n\n# Print the formatted VIF values\nprint(vif_df)","metadata":{"execution":{"iopub.status.busy":"2023-11-21T04:58:48.981242Z","iopub.execute_input":"2023-11-21T04:58:48.982710Z","iopub.status.idle":"2023-11-21T04:58:49.007936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"model1 <- lm(log(shares)~., data = df_1)\nsummary(model1)","metadata":{"execution":{"iopub.status.busy":"2023-11-21T04:58:49.010716Z","iopub.execute_input":"2023-11-21T04:58:49.012191Z","iopub.status.idle":"2023-11-21T04:58:49.194251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"residuals = model1$residuals\nhead(residuals)","metadata":{"execution":{"iopub.status.busy":"2023-11-21T04:58:49.200520Z","iopub.execute_input":"2023-11-21T04:58:49.204056Z","iopub.status.idle":"2023-11-21T04:58:49.248933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hist(model1$residuals, breaks = 5)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-21T04:58:49.256977Z","iopub.execute_input":"2023-11-21T04:58:49.260826Z","iopub.status.idle":"2023-11-21T04:58:49.393300Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot(model1)","metadata":{"execution":{"iopub.status.busy":"2023-11-21T04:58:49.396094Z","iopub.execute_input":"2023-11-21T04:58:49.397601Z","iopub.status.idle":"2023-11-21T04:58:57.771037Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model2 <- lm(sqrt(shares)~.,data = df_1)\n# summary(model2)","metadata":{"execution":{"iopub.status.busy":"2023-11-21T04:58:57.775289Z","iopub.execute_input":"2023-11-21T04:58:57.777650Z","iopub.status.idle":"2023-11-21T04:58:57.794428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stepwise.model <- stepAIC(model1, direction=\"backward\")","metadata":{"execution":{"iopub.status.busy":"2023-11-21T04:58:57.798621Z","iopub.execute_input":"2023-11-21T04:58:57.801043Z","iopub.status.idle":"2023-11-21T04:59:41.673943Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"summary(stepwise.model)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-21T04:59:41.679334Z","iopub.execute_input":"2023-11-21T04:59:41.683256Z","iopub.status.idle":"2023-11-21T04:59:41.773034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"colnames(df_1)","metadata":{"execution":{"iopub.status.busy":"2023-11-21T04:59:41.778989Z","iopub.execute_input":"2023-11-21T04:59:41.782978Z","iopub.status.idle":"2023-11-21T04:59:41.820746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Assuming data1 is your dataframe\ny <- df_1$shares  # Create a y dataset containing the \"shares\" column\nx <- df_1[,c(\n    \"n_tokens_title\", \n    \"n_unique_tokens\", \n    \"n_non_stop_unique_tokens\", \n    \"num_hrefs\", \n    \"num_self_hrefs\", \n    \"num_imgs\", \n    \"num_videos\", \n    \"average_token_length\", \n    \"num_keywords\", \n    \"data_channel_is_lifestyle\", \n    \"data_channel_is_entertainment\", \n    \"data_channel_is_bus\", \n    \"data_channel_is_socmed\", \n    \"data_channel_is_tech\", \n    \"data_channel_is_world\", \n    \"kw_min_min\", \n    \"kw_max_min\", \n    \"kw_max_max\", \n    \"self_reference_min_shares\", \n    \"self_reference_max_shares\", \n    \"weekday_is_monday\", \n    \"weekday_is_tuesday\", \n    \"weekday_is_wednesday\", \n    \"weekday_is_thursday\", \n    \"weekday_is_friday\", \n    \"global_subjectivity\", \n    \"min_positive_polarity\", \n    \"avg_negative_polarity\", \n    \"title_subjectivity\", \n    \"title_sentiment_polarity\", \n    \"abs_title_subjectivity\", \n    \"abs_title_sentiment_polarity\"\n)]\n","metadata":{"execution":{"iopub.status.busy":"2023-11-21T04:59:41.826300Z","iopub.execute_input":"2023-11-21T04:59:41.830200Z","iopub.status.idle":"2023-11-21T04:59:41.868969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y1 <-log(y)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-21T04:59:41.873714Z","iopub.execute_input":"2023-11-21T04:59:41.875719Z","iopub.status.idle":"2023-11-21T04:59:41.892295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x <- scale(x)\nhead(x)","metadata":{"execution":{"iopub.status.busy":"2023-11-21T04:59:41.896798Z","iopub.execute_input":"2023-11-21T04:59:41.898713Z","iopub.status.idle":"2023-11-21T04:59:42.017796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"set.seed(123)\nlasso <- cv.glmnet(x,y1,family=\"gaussian\", alpha=1)\nplot(lasso)","metadata":{"execution":{"iopub.status.busy":"2023-11-21T04:59:42.022159Z","iopub.execute_input":"2023-11-21T04:59:42.024021Z","iopub.status.idle":"2023-11-21T04:59:43.356120Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"summary(lasso)","metadata":{"execution":{"iopub.status.busy":"2023-11-21T04:59:43.359220Z","iopub.execute_input":"2023-11-21T04:59:43.360806Z","iopub.status.idle":"2023-11-21T04:59:43.384797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_lambda <- lasso$lambda.min\n# Make predictions using the best lambda\nlasso_predictions <- predict(lasso, newx = x, s = best_lambda)\n\n# Calculate Mean Squared Error (MSE)\nmse <- mean((lasso_predictions - y1)^2)\n\n# Calculate Root Mean Squared Error (RMSE)\nrmse <- sqrt(mse)\n\n# Calculate R-squared (R²)\nr_squared <- 1 - mse / var(y1)\n\n# Print or display the metrics\ncat(\"Mean Squared Error (MSE):\", mse, \"\\n\")\ncat(\"Root Mean Squared Error (RMSE):\", rmse, \"\\n\")\ncat(\"R-squared (R²):\", r_squared, \"\\n\")","metadata":{"execution":{"iopub.status.busy":"2023-11-21T04:59:43.387721Z","iopub.execute_input":"2023-11-21T04:59:43.389285Z","iopub.status.idle":"2023-11-21T04:59:43.471329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fit Ridge model\nset.seed(123)\nridge <- cv.glmnet(x, y1, family = \"gaussian\", alpha = 0)\n\n# Choose the best lambda (penalty parameter) based on cross-validation\nbest_lambda <- ridge$lambda.min\n\n# Make predictions using the best lambda\nridge_predictions <- predict(ridge, newx = x, s = best_lambda)\n\n# Calculate Mean Squared Error (MSE)\nmse_ridge <- mean((ridge_predictions - y1)^2)\n\n# Calculate Root Mean Squared Error (RMSE)\nrmse_ridge <- sqrt(mse_ridge)\n\n# Calculate R-squared (R²)\nr_squared_ridge <- 1 - mse_ridge / var(y1)\n\n# Print or display the metrics\ncat(\"Ridge Mean Squared Error (MSE):\", mse_ridge, \"\\n\")\ncat(\"Ridge Root Mean Squared Error (RMSE):\", rmse_ridge, \"\\n\")\ncat(\"Ridge R-squared (R²):\", r_squared_ridge, \"\\n\")\n","metadata":{"execution":{"iopub.status.busy":"2023-11-21T04:59:43.474682Z","iopub.execute_input":"2023-11-21T04:59:43.476396Z","iopub.status.idle":"2023-11-21T04:59:45.021291Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fit Elastic Net model\nset.seed(123)\nelastic_net <- cv.glmnet(x, y1, family = \"gaussian\", alpha = 0.5)\n\n# Choose the best lambda (penalty parameter) based on cross-validation\nbest_lambda <- elastic_net$lambda.min\n\n# Make predictions using the best lambda\nelastic_net_predictions <- predict(elastic_net, newx = x, s = best_lambda)\n\n# Calculate Mean Squared Error (MSE)\nmse_elastic_net <- mean((elastic_net_predictions - y1)^2)\n\n# Calculate Root Mean Squared Error (RMSE)\nrmse_elastic_net <- sqrt(mse_elastic_net)\n\n# Calculate R-squared (R²)\nr_squared_elastic_net <- 1 - mse_elastic_net / var(y1)\n\n# Print or display the metrics\ncat(\"Elastic Net Mean Squared Error (MSE):\", mse_elastic_net, \"\\n\")\ncat(\"Elastic Net Root Mean Squared Error (RMSE):\", rmse_elastic_net, \"\\n\")\ncat(\"Elastic Net R-squared (R²):\", r_squared_elastic_net, \"\\n\")\n","metadata":{"execution":{"iopub.status.busy":"2023-11-21T04:59:45.024158Z","iopub.execute_input":"2023-11-21T04:59:45.025635Z","iopub.status.idle":"2023-11-21T04:59:46.020030Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"[](http://)","metadata":{}},{"cell_type":"markdown","source":"# Simple Logestic Regression ","metadata":{}},{"cell_type":"code","source":"library(stats)\n# Assuming data1 is your dataframe\ny <- df_1$shares  # Create a y dataset containing the \"shares\" column\nx <- df_1[,c(\n    \"n_tokens_title\", \n    \"n_unique_tokens\", \n    \"n_non_stop_unique_tokens\", \n    \"num_hrefs\", \n    \"num_self_hrefs\", \n    \"num_imgs\", \n    \"num_videos\", \n    \"average_token_length\", \n    \"num_keywords\", \n    \"data_channel_is_lifestyle\", \n    \"data_channel_is_entertainment\", \n    \"data_channel_is_bus\", \n    \"data_channel_is_socmed\", \n    \"data_channel_is_tech\", \n    \"data_channel_is_world\", \n    \"kw_min_min\", \n    \"kw_max_min\", \n    \"kw_max_max\", \n    \"self_reference_min_shares\", \n    \"self_reference_max_shares\", \n    \"weekday_is_monday\", \n    \"weekday_is_tuesday\", \n    \"weekday_is_wednesday\", \n    \"weekday_is_thursday\", \n    \"weekday_is_friday\", \n    \"global_subjectivity\", \n    \"min_positive_polarity\", \n    \"avg_negative_polarity\", \n    \"title_subjectivity\", \n    \"title_sentiment_polarity\", \n    \"abs_title_subjectivity\", \n    \"abs_title_sentiment_polarity\"\n)]\n# Categorizing 'shares' into a binary variable for y\ny <- ifelse(y > 1400, 1, 0)\n\n# Combine x and y into a single dataframe\ndata <- cbind(x, high_engagement = y)\n\n# Splitting data into training and testing sets\nset.seed(123) # for reproducibility\nsplitIndex <- createDataPartition(data$high_engagement, p = .80, list = FALSE, times = 1)\ntrainData <- data[splitIndex,]\ntestData <- data[-splitIndex,]\n\n# Fitting the logistic regression model\nmodel <- glm(high_engagement ~ ., data = trainData, family = \"binomial\")\n\n# Summarizing the model\nsummary(model)\n\n# Making predictions on the test set\npredictions <- predict(model, newdata = testData, type = \"response\")\npredictions <- ifelse(predictions > 0.5, 1, 0)\n\n# Evaluating model performance\nconfusionMatrix <- table(Predicted = predictions, Actual = testData$high_engagement)\nprint(confusionMatrix)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-21T04:59:46.044131Z","iopub.execute_input":"2023-11-21T04:59:46.045621Z","iopub.status.idle":"2023-11-21T04:59:47.060198Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Assuming confusionMatrix is your confusion matrix\n\n# Extract counts from the confusion matrix\ntrue_positives <- confusionMatrix[2, 2]\ntrue_negatives <- confusionMatrix[1, 1]\nfalse_positives <- confusionMatrix[1, 2]\nfalse_negatives <- confusionMatrix[2, 1]\n\n# Calculating Metrics\naccuracy <- (true_positives + true_negatives) / (true_positives + true_negatives + false_positives + false_negatives)\nprecision <- true_positives / (true_positives + false_positives)\nrecall <- true_positives / (true_positives + false_negatives)\nf1_score <- 2 * (precision * recall) / (precision + recall)\n\n# Handling potential NaN values due to division by zero\nif (is.nan(precision)) {\n  precision <- 0\n}\nif (is.nan(recall)) {\n  recall <- 0\n}\nif (is.nan(f1_score)) {\n  f1_score <- 0\n}\n\n# Printing the Metrics\nprint(paste(\"Accuracy:\", accuracy))\nprint(paste(\"Precision:\", precision))\nprint(paste(\"Recall:\", recall))\nprint(paste(\"F1 Score:\", f1_score))\n","metadata":{"execution":{"iopub.status.busy":"2023-11-21T04:59:47.070679Z","iopub.execute_input":"2023-11-21T04:59:47.078083Z","iopub.status.idle":"2023-11-21T04:59:47.147242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Assuming you have these variables\nactuals <- testData$high_engagement\npredicted_probs <- predict(model, newdata = testData, type = \"response\")\n\n# Binarize predicted probabilities for calculating MAE\npredicted_binary <- ifelse(predicted_probs > 0.5, 1, 0)\n\n# Mean Squared Error (MSE)\nmse <- mean((actuals - predicted_probs) ^ 2)\n\n# Root Mean Squared Error (RMSE)\nrmse <- sqrt(mse)\n\n# Mean Absolute Error (MAE)\nmae <- mean(abs(actuals - predicted_binary))\n\n# Printing the Metrics\nprint(paste(\"MSE:\", mse))\nprint(paste(\"RMSE:\", rmse))\nprint(paste(\"MAE:\", mae))\n","metadata":{"execution":{"iopub.status.busy":"2023-11-21T04:59:47.150107Z","iopub.execute_input":"2023-11-21T04:59:47.151623Z","iopub.status.idle":"2023-11-21T04:59:47.251728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load necessary libraries\nlibrary(stats)\n\n# Assuming data1 is your dataframe\ny <- df_1$shares  # Create a y dataset containing the \"shares\" column\nx <- df_1[,c(\n    \"n_tokens_title\", \n    \"n_unique_tokens\", \n    \"n_non_stop_unique_tokens\", \n    \"num_hrefs\", \n    \"num_self_hrefs\", \n    \"num_imgs\", \n    \"num_videos\", \n    \"average_token_length\", \n    \"num_keywords\", \n    \"data_channel_is_lifestyle\", \n    \"data_channel_is_entertainment\", \n    \"data_channel_is_bus\", \n    \"data_channel_is_socmed\", \n    \"data_channel_is_tech\", \n    \"data_channel_is_world\", \n    \"kw_min_min\", \n    \"kw_max_min\", \n    \"kw_max_max\", \n    \"self_reference_min_shares\", \n    \"self_reference_max_shares\", \n    \"weekday_is_monday\", \n    \"weekday_is_tuesday\", \n    \"weekday_is_wednesday\", \n    \"weekday_is_thursday\", \n    \"weekday_is_friday\", \n    \"global_subjectivity\", \n    \"min_positive_polarity\", \n    \"avg_negative_polarity\", \n    \"title_subjectivity\", \n    \"title_sentiment_polarity\", \n    \"abs_title_subjectivity\", \n    \"abs_title_sentiment_polarity\"\n)]\n# Categorizing 'shares' into a binary variable for y\ny <- ifelse(y > 1400, 1, 0)\n\n# Combine x and y into a single dataframe\ndata <- cbind(x, high_engagement = y)\n\n# Set a random seed for reproducibility\nset.seed(123)\n\n# Split the data into training and testing sets\nsplitIndex <- createDataPartition(data$high_engagement, p = .70, list = FALSE, times = 1)\ntrainData <- data[splitIndex,]\ntestData <- data[-splitIndex,]\n\n# Fit the logistic regression model on the training set\nmodel <- glm(high_engagement ~ ., data = trainData, family = \"binomial\")\n\n# Make predictions on the training set\ntrain_predictions <- predict(model, newdata = trainData, type = \"response\")\ntrain_predictions <- ifelse(train_predictions > 0.5, 1, 0)\n\n# Evaluate training set performance\ntrain_confusionMatrix <- table(Predicted = train_predictions, Actual = trainData$high_engagement)\nprint(\"Training Set Confusion Matrix:\")\nprint(train_confusionMatrix)\n\n# Make predictions on the test set\ntest_predictions <- predict(model, newdata = testData, type = \"response\")\ntest_predictions <- ifelse(test_predictions > 0.5, 1, 0)\n\n# Evaluate test set performance\ntest_confusionMatrix <- table(Predicted = test_predictions, Actual = testData$high_engagement)\nprint(\"Test Set Confusion Matrix:\")\nprint(test_confusionMatrix)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-21T04:59:47.262373Z","iopub.execute_input":"2023-11-21T04:59:47.269711Z","iopub.status.idle":"2023-11-21T04:59:47.998991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load necessary libraries\nlibrary(stats)\nlibrary(boot)  # Load the boot library for cv.glm\n\n# Assuming data1 is your dataframe\ny <- df_1$shares  # Create a y dataset containing the \"shares\" column\nx <- df_1[,c(\n    \"n_tokens_title\", \n    \"n_unique_tokens\", \n    \"n_non_stop_unique_tokens\", \n    \"num_hrefs\", \n    \"num_self_hrefs\", \n    \"num_imgs\", \n    \"num_videos\", \n    \"average_token_length\", \n    \"num_keywords\", \n    \"data_channel_is_lifestyle\", \n    \"data_channel_is_entertainment\", \n    \"data_channel_is_bus\", \n    \"data_channel_is_socmed\", \n    \"data_channel_is_tech\", \n    \"data_channel_is_world\", \n    \"kw_min_min\", \n    \"kw_max_min\", \n    \"kw_max_max\", \n    \"self_reference_min_shares\", \n    \"self_reference_max_shares\", \n    \"weekday_is_monday\", \n    \"weekday_is_tuesday\", \n    \"weekday_is_wednesday\", \n    \"weekday_is_thursday\", \n    \"weekday_is_friday\", \n    \"global_subjectivity\", \n    \"min_positive_polarity\", \n    \"avg_negative_polarity\", \n    \"title_subjectivity\", \n    \"title_sentiment_polarity\", \n    \"abs_title_subjectivity\", \n    \"abs_title_sentiment_polarity\"\n)]\n# Categorizing 'shares' into a binary variable for y\ny <- ifelse(y > 1400, 1, 0)\n\n# Combine x and y into a single dataframe\ndata <- cbind(x, high_engagement = y)\n\n# Set a random seed for reproducibility\nset.seed(123)\n\n# Perform k-fold cross-validation (e.g., 5-fold)\nk <- 5  # Number of folds\ncv_results <- cv.glm(data, glmfit = glm(high_engagement ~ ., data = data, family = \"binomial\"), K = k)\n\n# Print the cross-validation results\n#print(cv_results)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-21T04:59:48.011185Z","iopub.execute_input":"2023-11-21T04:59:48.015285Z","iopub.status.idle":"2023-11-21T04:59:51.598119Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"library(pROC)","metadata":{"execution":{"iopub.status.busy":"2023-11-21T04:59:51.603006Z","iopub.execute_input":"2023-11-21T04:59:51.606378Z","iopub.status.idle":"2023-11-21T04:59:51.674998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate and print CV metrics\ncv_deviance <- cv_results$delta\ncv_mean_deviance <- mean(cv_deviance)\nprint(\"Cross-Validation Metrics:\")\nprint(paste(\"Mean Deviance:\", cv_mean_deviance))\n\n# Fit the logistic regression model on the training set\nmodel <- glm(high_engagement ~ ., data = trainData, family = \"binomial\")\n\n# Make predictions on the training set\ntrain_predictions <- predict(model, newdata = trainData, type = \"response\")\ntrain_predictions <- ifelse(train_predictions > 0.5, 1, 0)\n\n# Evaluate training set performance\ntrain_confusionMatrix <- table(Predicted = train_predictions, Actual = trainData$high_engagement)\n\n# Calculate training set metrics\ntrain_accuracy <- sum(diag(train_confusionMatrix)) / sum(train_confusionMatrix)\ntrain_precision <- train_confusionMatrix[2, 2] / sum(train_confusionMatrix[, 2])\ntrain_recall <- train_confusionMatrix[2, 2] / sum(train_confusionMatrix[2, ])\ntrain_f1 <- 2 * (train_precision * train_recall) / (train_precision + train_recall)\n\n# Calculate AUC-ROC for training set\ntrain_roc <- roc(trainData$high_engagement, train_predictions)\ntrain_auc <- auc(train_roc)\n\nprint(\"Training Set Metrics:\")\nprint(paste(\"Accuracy:\", train_accuracy))\nprint(paste(\"Precision:\", train_precision))\nprint(paste(\"Recall:\", train_recall))\nprint(paste(\"F1-Score:\", train_f1))\nprint(paste(\"AUC-ROC:\", train_auc))\n\n# Make predictions on the test set\ntest_predictions <- predict(model, newdata = testData, type = \"response\")\ntest_predictions <- ifelse(test_predictions > 0.5, 1, 0)\n\n# Evaluate test set performance\ntest_confusionMatrix <- table(Predicted = test_predictions, Actual = testData$high_engagement)\n\n# Calculate test set metrics\ntest_accuracy <- sum(diag(test_confusionMatrix)) / sum(test_confusionMatrix)\ntest_precision <- test_confusionMatrix[2, 2] / sum(test_confusionMatrix[, 2])\ntest_recall <- test_confusionMatrix[2, 2] / sum(test_confusionMatrix[2, ])\ntest_f1 <- 2 * (test_precision * test_recall) / (test_precision + test_recall)\n\n# Calculate AUC-ROC for test set\ntest_roc <- roc(testData$high_engagement, test_predictions)\ntest_auc <- auc(test_roc)\n\nprint(\"Test Set Metrics:\")\nprint(paste(\"Accuracy:\", test_accuracy))\nprint(paste(\"Precision:\", test_precision))\nprint(paste(\"Recall:\", test_recall))\nprint(paste(\"F1-Score:\", test_f1))\nprint(paste(\"AUC-ROC:\", test_auc))","metadata":{"execution":{"iopub.status.busy":"2023-11-21T04:59:51.680172Z","iopub.execute_input":"2023-11-21T04:59:51.683794Z","iopub.status.idle":"2023-11-21T04:59:52.519695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Ensure that you have the pROC package\n# install.packages(\"pROC\") # Uncomment this line if you haven't installed the package\n\n# Load the pROC package\nlibrary(pROC)\n\n# Assuming you have the actual values and predicted probabilities\n# actuals <- testData$high_engagement\n# predicted_probs <- predict(model, newdata = testData, type = \"response\")\n\n# Generate the ROC object\nroc_obj <- roc(actuals, predicted_probs)\n\n# Plotting the ROC curve\nplot(roc_obj, main=\"ROC Curve\", col=\"#1c61b6\", lwd=2)\nabline(a=0, b=1, lty=2, col=\"red\") # Adding a reference line\n","metadata":{"execution":{"iopub.status.busy":"2023-11-21T04:59:52.522471Z","iopub.execute_input":"2023-11-21T04:59:52.523921Z","iopub.status.idle":"2023-11-21T04:59:52.677595Z"},"trusted":true},"execution_count":null,"outputs":[]}]}